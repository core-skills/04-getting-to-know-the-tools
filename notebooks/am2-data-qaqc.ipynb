{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping and reshaping data\n",
    "\n",
    "We're going to look at some different ways of grouping and aggregating data. We're building towards thinking about 'split', 'apply', and 'combine workflows, which look something like this:\n",
    "\n",
    "![split-apply-combine](https://github.com/core-skills/02-getting-to-know-the-tools/blob/master/notebooks/split-apply-combine.png?raw=true)\n",
    "\n",
    "(taken from Jake VanderPlas' excellent [Python data science handbook](https://github.com/jakevdp/PythonDataScienceHandbook) - check out all the notebooks available on github if you want more in-depth examples than what we've worked through today).\n",
    "\n",
    "## Groupby\n",
    "\n",
    "Find the pandas `groupby` method and work out how it works on your dataframe. Hint: try passing a categorical column from your data. \n",
    "\n",
    "If you don't have a categorical column but you do have a column of numbers, you can generate groups by binning the data into seperate bins using the `pandas.cut` function - something like this:\n",
    "\n",
    "```python\n",
    "import pandas\n",
    "from numpy import inf\n",
    "from random_data import random_dataframe\n",
    "\n",
    "# Our faithful bogus dataframe\n",
    "df = random_dataframe(30)\n",
    "\n",
    "# Add a new column which bins the a values\n",
    "df['how_big'] = pandas.cut(df.a, \n",
    "                           bins=[-inf, 50, inf],\n",
    "                           labels=('low', 'high'))\n",
    "```\n",
    "\n",
    "`pandas.cut` can often be useful for investigating subsets of numerical data (e.g. ore grade in marginal blocks!).\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next try to generate some summary statistics about each of your groups. The `info` and `describe` methods of the pandas dataframe are good places to start - try something like this:\n",
    "\n",
    "```python\n",
    "from random_data import random_dataframe\n",
    "\n",
    "# Yet more bogus data\n",
    "df_rand = random_dataframe(100)\n",
    "\n",
    "# Iterating in a for-loop\n",
    "for category, grp_df in df_rand.groupby('category'):\n",
    "    print(f\"\\nInfo for group {category}\")\n",
    "    print(grp_df.describe())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to iterate over the groups if you don't want to - you can pipeline them to an aggregating function directly (which is often easier to read).\n",
    "\n",
    "```python\n",
    "# Calculating an aggregation directly\n",
    "df.groupby('category').sum()\n",
    "```\n",
    "\n",
    "Try looking at some of the other pandas aggregations: `count`, `first`, `last`, `mean`, `median`, `min`, `max`, `std`, `var`, `mad`, `prod`, `sum`. What do each of these do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some of the more advanced group options - for example you can set a category as an index, and pass functions which take an index and output a group.\n",
    "\n",
    "```python\n",
    "# Calculating an aggregation by specifying a mapping \n",
    "# from index to group\n",
    "mapping = {'x': 'first', 'y': 'first', 'z': 'second'}\n",
    "df.set_index('category').groupby(mapping).mean()\n",
    "```\n",
    "\n",
    "How might you write a small function to start to aggregate or summarize the data in your data's groups in more complex ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot table\n",
    "\n",
    "Pivot tables are a lot like groupby operations but instead of ending up with one column of groups we can end up with multidimensional arrays of aggregations. \n",
    "\n",
    "In the diagram at the top of the page, you can think of a pivot table splitting the data using more than one column in the 'split' step.\n",
    "\n",
    "This is generally more useful when we want to start to aggregate along multiple dimensions.\n",
    "\n",
    "Using our example from above:\n",
    "\n",
    "```python\n",
    "import pandas\n",
    "from numpy import inf\n",
    "from random_data import random_dataframe\n",
    "\n",
    "# Our faithful bogus dataframe\n",
    "df = random_dataframe(3000, categories='uvwxyz')\n",
    "\n",
    "# Add a new column which bins the a values\n",
    "df['how_big'] = pandas.cut(df.a, \n",
    "                           bins=[-inf, 50, inf],\n",
    "                           labels=('low', 'high'))\n",
    "\n",
    "# make a new pivot table that calculates the mean for each\n",
    "# of our subcategories - both 'x,y,z' and 'low' and 'high'\n",
    "pivot = df.pivot_table('b', index='category', columns='how_big', aggfunc='mean')\n",
    "```\n",
    "\n",
    "Try creating a pivot table on your own data (as before you can use `pandas.cut` to bin numerical data if that's more useful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from numpy import inf\n",
    "from random_data import random_dataframe\n",
    "\n",
    "# Our faithful bogus dataframe\n",
    "df = random_dataframe(3000, categories='uvwxyz')\n",
    "\n",
    "# Add a new column which bins the a values\n",
    "df['how_big'] = pandas.cut(df.a, \n",
    "                           bins=[-inf, 25, 50, 75, inf],\n",
    "                           labels=('tiny', 'small', 'medium' 'large', 'huge'))\n",
    "\n",
    "# make a new pivot table that calculates the mean for each\n",
    "# of our subcategories - both 'x,y,z' and 'low' and 'high'\n",
    "pivot = df.pivot_table('b', index='category', columns='how_big', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('how_big').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data\n",
    "\n",
    "Next we're going to use [seaborn](seaborn.pydata.org) to generate some pretty plots of our data. \n",
    "\n",
    "Most Python tutorials will introduce [matplotlib](https://matplotlib.org) at this stage because it's the default but seaborn is a much higher-level library with a nicer API, especially for exploratory vis (matplotlib will probably make more sense to you if you're coming from MATLAB world though). The only hangover is that we need to include the `%matplotlib inline` cell magic to tell Jupyter to render the graphics inline for us. \n",
    "\n",
    "We'll start by looking at our random dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "from numpy import inf\n",
    "import pandas as pd\n",
    "\n",
    "sns.set()\n",
    "\n",
    "from random_data import random_dataframe\n",
    "\n",
    "# Set up our dataframe and pivot table\n",
    "df = random_dataframe(3000, categories='uvwxyz')\n",
    "df['how_big'] = pd.cut(df.a, \n",
    "                           bins=[-inf, 25, 50, 75, inf],\n",
    "                           labels=('tiny', 'small', 'medium' 'large', 'huge'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one-dimensional dataset we can try `seaborn.distplot`, `seaborn.kdeplot` and `seaborn.rugplot` to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_context('talk') # paper, notebook, talk, poster\n",
    "# sns.set_palette('colorblind') # deep, muted, pastel, bright, dark, and colorblind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, analagous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(df['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(df['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `jointplot` to generate a scatter and histograms of sets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='a', y='b', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='a', y='b', data=df, kind='hex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For times when you want 'plot everything against everything else' you can do something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really useful for pulling out relationships between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c'] = df.a + df.b * df.a\n",
    "df['d'] = df.c * df.b + df.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.PairGrid(df)\n",
    "# g.map_diag(sns.kdeplot)\n",
    "# g.map_offdiag(sns.kdeplot, n_levels=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn also has a heap of support for categorical data. We can also include more dimensions in the visualization by specifying further dimensions as colors or point size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x='a', y='how_big', hue='category', data=df, jitter=True, dodge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the pivot table we generated above with heatmaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = df.pivot_table('b', index='category', columns='how_big', aggfunc='sum')\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn can get a lot more complicated than this and it's worth digging through the examples to find useful ways of slicing and dicing your dataframes into pictures.\n",
    "\n",
    "Now try this out on your own dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
